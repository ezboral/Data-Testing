---
title: "Machine Learning"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

``` {r, warning = FALSE, message = FALSE}
library(readr)
library(caTools)
library(ROCR)
dcc <- read_csv("~/dcc.csv")
```

``` {r eval = TRUE, echo = TRUE}
# Change column names from X1, X2 etc to the value in row 2
colnames(dcc) <- dcc[1,]

# Delete row 2 to remove duplicate titles
dcc = dcc[-1,]

# Remove ID column (column #1)
dcc <- dcc[-1]

# Clean up column names
names(dcc) <- c("LimitAmt", "Gender", "Education", "Marriage", "Age", "StatusSep05", "StatusAug05"
                , "StatusJul05", "StatusJun05", "StatusMay05", "StatusApr05", "BalSep05", "BalAug05",
                "BalJul05", "BalJun05", "BalMay05", "BalApr05", "PayAmtSep05", "PayAmtAug05",
                "PayAmtJul05", "PayAmtJun05", "PayAmtMay05", "PayAmtApr05", "DefaultOct05")
```

The data set needs to be transformed into a data frame.

``` {r eval = TRUE, echo = TRUE}
# Transform data set to data frame
dcc <- as.data.frame(dcc)
```


``` {r eval = TRUE, echo = TRUE}
# Convert 4,5, and 6 to 0 as they all represent "Other"
dcc$Education[dcc$Education == 4] <- 0
dcc$Education[dcc$Education == 5] <- 0
dcc$Education[dcc$Education == 6] <- 0
```

``` {r eval = TRUE, echo = TRUE}
# Convert columns in data set to numeric
dcc[] <- lapply(dcc, function(x) as.numeric(x))
```

## Data Analysis

In this section we will use machine learning to build a logistic regression model. This model will predict the probability of either a default or no default using the independent variables in our data set.

#### Setting a Baseline Rate

We saw earlier that the default rate across all loans is 22.12%, which means that 77.88% of loans are not defaulting.  Our goal is to improve upon the baseline with our logistic regression model.

#### Building the model

We first split the data into a training data set and testing data set. We will use the training set to discover potentially predictive relationships and use the testing set to assess the performance of these relationships. 

Our training set uses 75% of the observations, and our testing set uses 25%. The set.seed variable is used to make sure the dependent variable (default) is well balanced in both the training and testing sets.  

``` {r eval = TRUE, echo = TRUE}
# Split data into training and testing set with a 75/25 ratio
set.seed(64)
split = sample.split(dcc$DefaultOct05, SplitRatio = 0.75)
dccTrain = subset(dcc, split == TRUE)
dccTest = subset(dcc, split == FALSE)
```

``` {r eval = TRUE, echo = TRUE}
# Check rows for both the training and testing set
nrow(dccTrain)
nrow(dccTest)
```

We see there are 22,500 rows in the training data set and 7,500 in the testing data set which is consistent with a 75/25 split.

We will now build our model using the training data set. We will start with a large universe of variables and continue to refine our model by removing those with no significance.

##### Model1

``` {r eval = TRUE, echo = TRUE}
# Model 1 starts with almost all the independent variables
dccLog1 = glm(DefaultOct05 ~ LimitAmt + Gender + Education + Marriage + Age +
                StatusSep05 + StatusAug05 + StatusJul05 + StatusJun05 + StatusMay05 +
                StatusApr05 + PayAmtSep05 + PayAmtAug05 + PayAmtMay05 + PayAmtApr05 +
                BalSep05 + BalAug05 + BalJul05 + BalJun05 + BalMay05 + BalApr05, 
              data=dccTrain, family=binomial)

summary(dccLog1)
```

We see that 7 variables are not significant. We will remove the variables with no significance and rerun the model. 

##### Model2

``` {r eval = TRUE, echo = TRUE}
# Model 2 removes insignificant variables
dccLog2 = glm(DefaultOct05 ~ LimitAmt + Gender + Marriage + Age +
                StatusSep05 + StatusAug05 + StatusJul05 + PayAmtSep05 + PayAmtAug05 + 
                PayAmtMay05 + BalSep05 + BalAug05 + BalMay05 + BalApr05, 
              data=dccTrain, family=binomial)

summary(dccLog2)
```

We see an improvement in the AIC score from 20935 to 20933, but one of the remaining variables is not significant. Once again, we will remove this variable and rerun the model.

##### Model3

``` {r eval = TRUE, echo = TRUE}
# Model 3 removes one more that is insignificant
dccLog3 = glm(DefaultOct05 ~ LimitAmt + Gender + Marriage + Age +
                StatusSep05 + StatusAug05 + StatusJul05 + PayAmtSep05 + PayAmtAug05 + 
                PayAmtMay05 + BalSep05 + BalAug05 + BalApr05, 
              data=dccTrain, family=binomial)

summary(dccLog3)
```

The April Balance amount is insignificant.  After removing this variable we run our model once more, however the AIC score is slightly higher (moving from 20933 to 20934)

##### Model4

``` {r eval = TRUE, echo = TRUE}
# Model 4 shows all remaining variables with significance
dccLog4 = glm(DefaultOct05 ~ LimitAmt + Gender + Marriage + Age +
                StatusSep05 + StatusAug05 + StatusJul05 + PayAmtSep05 + PayAmtAug05 + 
                PayAmtMay05 + BalSep05 + BalAug05, 
              data=dccTrain, family=binomial)

summary(dccLog4)
```

Model 4 appears to be the best mix of independent variables as it delivers an AIC score of 20934, and shows significance of between 0 and 0.001 or between 0.001 and 0.01 for all independent variables. The coefficients for Age, payment status, and the August balance variable are positive, which indicates that higher values are indicative of a higher chance of default.  

#### Predict the data

By using a threshold value, we can convert our probabilities to predictions.  If the probability of default is greater than the threshold, then we predict default. If its below, then we predict no default.  The threshold we start with is based on what type of error we are more comfortable with.  A high threshold will lead to fewer innacurately predicted defaults but more innacurate predictions of a loan non defaulting. A low threshold will lead to the opposite. If there is no preference between the two errors we can choose a threshold of 0.5 since it is in between our two outcomes (0 or 1).

Using a threshold of 0.5, lets look at how our model works with the test data set.

``` {r eval = TRUE, echo = TRUE}
# Prediction on Test data
predictdccTest = predict(dccLog4, type="response", newdata = dccTest)
table(dccTest$DefaultOct05, predictdccTest > 0.5)
```

The matrix above shows the breakdown of of all defaults.  The rows are the acutal result (0 = no default, 1 = default), the columns are the predictions. We see that out of 7,500 occurrences, 5,682 loans did not default as predicted.  However, 1251 loans did default that were predicted to be paid. We also see that of the loans predicted to default, 408 were correctly predicted while 159 were incorrectly predicted to do so.

Our overall accuracy measures the number of correct predictions divided by the total number of loans.

``` {r eval = TRUE, echo = TRUE}
# Check rows for both the training and testing set
TestAccuracy = (5682 + 408)/(5682 + 408 + 1251 + 159)
TestAccuracy
```

The accuracy of the model is 81.2%.

This tells us that our model is a better predictor of default compared to a random guess, or our baseline (77.88%).

The sensitivity of our model is a measure of the true defaults divided by all defaults (both correct and incorrect predictions). Our sensitivity is 24.6% (408/(1251+408)).

The specificity is a measure of true non defaults divided by all non-defaults. Our specificity is 97.3% (5682/(5682 + 159)).

What these two measures tell us is we have a high specificity, but a very low sensitivity. With a 0.5 threshold, we are predicing that a relatively high number of loans are safe, but they actually default (1251 loans).  

#### Selecting a threshold

Since our project is attempting to help financial institutions predict default, the worst outcome is if we identify a loan as safe, but it actually defaults.  A loan that is predicted to default, but is actually safe is much less of a problem for that institution.  Therefore, we are concerned with keeping our false negative rate very low.

In this section, we will look at different threshold values and how those thresholds change the accuracy and various error rates for the model.

Below are a number of different thresholds ranging from 0.1 to 0.5.

``` {r eval = TRUE, echo = TRUE}
# Check 
table(dccTest$DefaultOct05, predictdccTest > 0.1)
table(dccTest$DefaultOct05, predictdccTest > 0.2)
table(dccTest$DefaultOct05, predictdccTest > 0.3)
table(dccTest$DefaultOct05, predictdccTest > 0.5)
```

We see the results of using different thresholds.  The table below displays the results in a more visually appealing format.

``` {r eval = TRUE, echo = TRUE}
# Check 
ThreshTable <- matrix(c(36.6, 61.7, 80.44, 81.2, 89.9, 71.9, 44.9, 24.6, 21.4, 58.8, 90.5, 97.3, 11.8,
                        12.0, 14.8, 18.0), ncol = 4, byrow= FALSE)
colnames(ThreshTable) <- c("Accuracy", "Sensitivity", "Specificity", "False Negative")
rownames(ThreshTable) <- c("t=0.1", "t=0.2", "t=0.3", "t=0.5")
ThreshTable <- as.table(ThreshTable)
ThreshTable
```

The false negative rate decreases as the threshold decreases, but the accuracy of our model also decreases. 

At thresholds of 0.1 and 0.2 the model's accuracy is too low (36.6% and 61.7%).  However, a threshold of 0.3 boosts accuracy to above 80% and the false negative rate is under 15%. The sensitivity is pretty low at 44.90%, but we are willing to sacrifice in this area as our false negative rate can be much more costly for a financial institution.


We can also visualize the same analysis with a ROC curve.  The graph below shows where the trade off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) lies.  

``` {r eval = TRUE, echo = TRUE}
# Check 
ROCRpreddcc = prediction(predictdccTest, dccTest$DefaultOct05)
ROCRperfdcc = performance(ROCRpreddcc, "tpr", "fpr")
plot(ROCRperfdcc, colorize=TRUE, print.cutoffs.at=seq(0,1,0.1), text.adj=c(-0.2, 1.7))
```

We can see that a threshold of around 0.3 gives the best tradeoff 

## Conclusions

